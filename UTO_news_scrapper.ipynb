{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **USA Today News Scraper**"
      ],
      "metadata": {
        "id": "1n7W2zCThFOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import bs4\n",
        "import re\n",
        "\n",
        "import os\n",
        "import csv\n",
        "\n",
        "# CSV file path containing links\n",
        "csv_file = '/content/UTO_news_links.csv'\n",
        "\n",
        "# output folder path\n",
        "output_folder = '/content/UTO_scraped_texts'  # You can modify this path\n",
        "\n",
        "# Function to scrape data from a given link and save it in a text file\n",
        "def scrape_and_save(link, serial_number, output_folder):\n",
        "    response = requests.get(link)\n",
        "    soup = bs4.BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Extract the desired texts\n",
        "    # Date\n",
        "    # Find the div element with the specified class name\n",
        "    date_div = str(soup.find('div', class_='gnt_ar_dt'))\n",
        "\n",
        "    pattern = r\"\\w+\\s\\d{1,2},\\s\\d{4}\"\n",
        "    date_match = re.search(pattern, date_div)\n",
        "\n",
        "    if date_match:\n",
        "      date = date_match.group(0)\n",
        "      # print(date)  # Output: May 24, 2022\n",
        "    else:\n",
        "      date = \"\"\n",
        "\n",
        "    # Headline\n",
        "    headline_element = soup.find('h1', class_='gnt_ar_hl') or soup.find('h1', class_='gnt_sv_hl')\n",
        "    headline = headline_element.get_text() if headline_element else \"\"\n",
        "    headline = headline.strip() + \".\"  # Add a period at the end\n",
        "\n",
        "    # # Sub-headline\n",
        "    # sub_headline_element = soup.find('h2', class_='dmnc_generic-header-header-module__sF01k secondaryRoman secondaryRoman-30 md_secondaryRoman-40 text-gray-dark')\n",
        "    # sub_headline = sub_headline_element.get_text() if sub_headline_element else \"\"\n",
        "\n",
        "    # Paragraphs\n",
        "    content_element = soup.find('div', class_='gnt_ar_b') or soup.find('div', class_='articleBody') or soup.find('div', class_='gnt_sv_vb')\n",
        "    p_tags = content_element.find_all('p') if content_element else []\n",
        "\n",
        "    # Photo captions\n",
        "    # caption_content = soup.find_all('figcaption', class_='dmnc_images-image-elements-module__ku669 secondaryRoman secondaryRoman-10 mt-3 text-gray-medium')\n",
        "    # caption = [fig.get_text() for fig in caption_content] if caption_content else []\n",
        "\n",
        "    # Create the output folder if it doesn't exist\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Save the texts in a separate text file within the output folder\n",
        "    filename = f\"{serial_number}_{link.replace('/', '_').replace(':', '')}.txt\"\n",
        "    file_path = os.path.join(output_folder, filename)\n",
        "    with open(file_path, 'w', encoding='utf-8') as file:\n",
        "        file.write(date + '\\n')\n",
        "        file.write(headline + '\\n')\n",
        "        # file.write(sub_headline + '\\n')\n",
        "\n",
        "        # Write the content of the p_tags\n",
        "        for p in p_tags:\n",
        "            file.write(p.get_text() + '\\n')\n",
        "\n",
        "# Read the links from the CSV file [columns for both serial_number and link are explicitly specified]\n",
        "with open(csv_file, 'r') as file:\n",
        "    reader = csv.reader(file)\n",
        "    next(reader) # Skip the header row\n",
        "    # links = list(reader)\n",
        "    for row in reader:\n",
        "      serial_number = row[0]  # Specify the column number for the serial number\n",
        "      link = row[1]  # Specify the column number for the link\n",
        "      scrape_and_save(link, serial_number, output_folder)\n"
      ],
      "metadata": {
        "id": "vr2BIOH3hEZu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fWVVoLx7w8xj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}